# -*- coding: utf-8 -*-
"""transd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Wk-n40Ds8U4LWJFo0yWUqvaEqHtWr_t
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class TransD(nn.Module):
    def __init__(self, entity_count, relation_count, device, norm=1, dim=100, margin=1.0):
        super(TransD, self).__init__()
        self.entity_count = entity_count
        self.relation_count = relation_count
        self.device = device
        self.norm = norm
        self.dim = dim
        self.margin = margin
        self._init_enitity_emb()
        self._init_relation_emb()
        self._init_entity_proj_emb()
        self._init_rel_proj_emb()

    def _init_enitity_emb(self):
        self.entity_embedding = nn.Embedding(num_embeddings=self.entity_count + 1,
                                             embedding_dim=self.dim,
                                             padding_idx=self.entity_count)
        torch.nn.init.xavier_normal_(self.entity_embedding.weight)

    def _init_relation_emb(self):
        self.relation_embedding = nn.Embedding(num_embeddings=self.relation_count + 1,
                                               embedding_dim=self.dim,
                                               padding_idx=self.relation_count)
        torch.nn.init.xavier_normal_(self.relation_embedding.weight)

    def _init_entity_proj_emb(self):
        self.entity_proj_emb = nn.Embedding(num_embeddings=self.entity_count + 1,
                                               embedding_dim=self.dim,
                                               padding_idx=self.relation_count)
        torch.nn.init.xavier_normal_(self.entity_proj_emb.weight)

    def _init_rel_proj_emb(self):
        self.relation_proj_embedding = nn.Embedding(num_embeddings=self.relation_count + 1,
                                               embedding_dim=self.dim,
                                               padding_idx=self.relation_count)
        torch.nn.init.xavier_normal_(self.relation_proj_embedding.weight)

    def forward(self, positive_triplets: torch.LongTensor, negative_triplets: torch.LongTensor):
        """Return model losses based on the input.
        :param positive_triplets: triplets of positives in Bx3 shape (B - batch, 3 - head, relation and tail)
        :param negative_triplets: triplets of negatives in Bx3 shape (B - batch, 3 - head, relation and tail)
        :return: tuple of the model loss, positive triplets loss component, negative triples loss component
        """
        assert positive_triplets.size()[1] == 3
        assert negative_triplets.size()[1] == 3

        self.entity_embedding_normalized = F.normalize(self.entity_embedding.weight, p=2, dim=1)
        self.relation_embedding_normalized = F.normalize(self.relation_embedding.weight, p=2, dim=1)
        self.entity_proj_emb_normalized = F.normalize(self.entity_proj_emb.weight, p=2, dim=1)
        self.relation_proj_embedding_normalized = F.normalize(self.relation_proj_embedding.weight, p=2, dim=1)

        score_pos = self._get_scores(positive_triplets)
        score_neg = self._get_scores(negative_triplets)

        loss = F.relu(self.margin + score_pos - score_neg).sum()
        return loss, score_pos, score_neg

    # def project(self, ent, e_proj_vect, r_proj_vect):
    #     e_proj_vect = torch.transpose(e_proj_vect, 0 ,1)
    #     ent = torch.transpose(ent, 0 ,1)
    #     # proj_e = matmul(proj_mat, ent.view(-1, self.ent_emb_dim, 1))
    #     # print(ent.size(), r_proj_vect.size(), e_proj_vect.view(-1, self.dim).size())
    #     # return ent*(r_proj_vect*e_proj_vect.view(-1, 1)+torch.eye(self.dim, self.dim))
    #     return torch.matmul(ent, (torch.matmul(r_proj_vect, e_proj_vect)+torch.eye(r_proj_vect.size()[0]).to('cuda')))

    def project(self, ent, e_proj_vect, r_proj_vect):
        """We note that :math:`p_r(e)_i = e^p^Te \\times r^p_i + e_i` which is
        more efficient to compute than the matrix formulation in the original
        paper.
        """
        b_size = ent.shape[0]

        scalar_product = (ent * e_proj_vect).sum(dim=1)
        proj_e = (r_proj_vect * scalar_product.view(b_size, 1))
        return proj_e + ent[:, :self.dim]

    def _get_scores(self, triplets):
        # triplets_embeddings = torch.stack((self.entity_embedding_normalized[triplets[:, 0]],
        #                                    -self.entity_embedding_normalized[triplets[:, 2]],
        #                                    self.relation_embedding_normalized[triplets[:, 1]]), dim=1)
        # score = triplets_embeddings.sum(dim=1).norm(p=self.norm, dim=-1)
        h = F.normalize(self.entity_embedding(triplets[:, 0]), p=2, dim=1)
        t = F.normalize(self.entity_embedding(triplets[:, 2]), p=2, dim=1)
        r = F.normalize(self.relation_embedding(triplets[:, 1]), p=2, dim=1)

        h_proj_v = F.normalize(self.entity_proj_emb(triplets[:, 0]), p=2, dim=1)
        t_proj_v = F.normalize(self.entity_proj_emb(triplets[:, 2]), p=2, dim=1)
        r_proj_v = F.normalize(self.relation_proj_embedding(triplets[:, 1]), p=2, dim=1)

        proj_h = self.project(h, h_proj_v, r_proj_v)
        proj_t = self.project(t, t_proj_v, r_proj_v)

        return - ((proj_h + r)-proj_t).norm(p=2, dim=-1)**2